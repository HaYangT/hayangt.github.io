---
title: "실시간 음성 필터링의 어려움과 그 대안"
date: 2026-02-12
categories: [Blog]
tags: [프로젝트]
pin: false
---

# 개요

내가 SSAFY에서 처음 진행한 프로젝트는 상담사 보호 및 보조를 위한 프로그램이였다. 평소 폭언과 같은 많은 스트레스를 받는 상담사들을 위해 우리는 상대방의 말을 필터링하는 기능을 넣기로 했다. 

실시간 상담환경에서는 수 초의 지연만으로도 잘못된 차단은 정상 고객을 공격자로 오인하는 문제가 된다. 반대로 폭언을 놓치면 상담사 보호라는 목적 자체가 무너진다.

 이 기능을 구현하기 위해 많은 우여곡절이 있었고, 오늘 이 일들을 모두 정리하고자 글을 쓴다.

---

## 첫 번째 시도

처음에는 특정한 단어(욕설 혹은 부적절한 말들)들을 필터링 하려고 했다. 상대방의 음성에서 단어가 검출되면, 그때 삐- 소리를 넣거나 해당 부분의 음성을 차단하려고 하였다. 이걸 어떻게 검출할까 고민하던 도중, STT를 사용해서 상대의 음성을 텍스트로 만든 뒤, 필터링을 해보면 어떨까 하는 생각이 들어 STT 모델들을 찾아서 테스트하기 시작했다. 우리는 실시간으로 통화를 해야했으므로, 빠르고 가벼운 모델이 필요했다. 그래서 이것저것 찾아보다가 Open AI의 Whisper의 성능이 괜찮다고 들어 바로 테스트를 시작했다.

FastAPI를 사용하여 로컬 서버에서 테스트를 했다. WebRTC로 들어오는 실시간 음성 스트림을 받아서 특정 시간 단위로 STT를 돌려 다시 클라이언트로 전송하는 방식이다.

Whisper에도 여러가지 모델이 있었다. 대본을 정해두고 그것을 읽으며 정확도를 분석했다.

| 모델 | 정확도 | 평균 지연 속도 (초) | 환각 |
| --- | --- | --- | --- |
| tiny | 0.64 | 0.8 | 적음 |
| base | 0.71 | 1.1 | 적음 |
| small | 0.70 | 1.4 | 적음 |
| medium | 0.68 | 2.2 | 보통 |
| large | 0.68 | 3.3 | 심함 |
| turbo | 0.65 | 3.4 | 심함 |

정확도는 사전에 정의한 문장 스크립트 대비 STT 결과의 핵심 키워드 일치 여부로 측정하였다.

모델이 커져도 로컬 환경에서 돌린 탓인지 지연속도에 비해 정확도는 더 떨어지는 경향이 보였다. 

지연속도 대비 정확도가 높은 base와 small을 사용할 모델로 선택하기로 했다.

하지만 정확도가 0.7이란 것은 30%는 틀린다는 것이다. 이는 우리 프로젝트에 치명적이였으며, 이걸 어떻게든 보완할 방법이 필요했다. 

당장 머릿속에 떠오른건 두가지였는데, 첫 번째는 잡음을 제거하여 목소리의 선명도를 높이는 것이였고, 두 번째는 Whisper에 프롬프트를 주어 어느정도 보정하는 것이였다.

이 때 당시 내가 준 프롬프트는 다음과 같다.

```
”들어오는 음성은 상담을 받으려고 하는 사람이다.”

“한국어를 사용한다.”

“상품의 고장에 대해 상담을 받으려고 한다.”
```

이 두가지 과정을 거치고 난뒤, 정확도가 대략 0.75정도로 올라왔지만 여전히 사용하기 힘든 정도였다. 

그래서 난 다른 방법을 생각해보기로 했다.

---

## 두 번째 시도

Whisper의 정확도가 계속 낮은 이유에 대해 생각해봤는데, 우리의 개발 환경은 Whisper를 사용하기 적합한 환경이 아니란 것을 깨달았다. 어느정도의 GPU가 있어야 속도와 성능이 나타나지만, 우리는 그럴 여건이 되지 않았다. 그래서 다른 방법을 찾던 도중, Web Speach API에 대해 알게 되었다.

Web Speech API는 브라우저에서 음성 인식과 음성 합성을 제공하는 Web API로, 웹 앱 안에서 음성 데이터를 처리할 수 있도록 해주는 API다.

나는 [이 글](https://developer.mozilla.org/ko/docs/Web/API/Web_Speech_API)을 보고 배웠다.


Web Speech API는 브라우저 의존성이 있어 특정 브라우저 외에서는 작동을 안한다는 단점이 있었지만, 그걸 감안하고도 매력적인 속도와 정확도를 보여주었으며 모바일 환경에서도 잘 작동했다. MVP 단계에서는 상담사 보호가 더 중요하다 판단하여 Web Speech API를 프로젝트에 적용하기로 했다. 

원래는 상담사측 클라이언트에서 들어오는 음성과 나가는 음성을 통해 화자분리를 구현하려고 했다. 고객과의 상담은 1대1이기 때문에 이 두가지로 구분이 쉽게 될줄 알았다. 그러나 실제 구현 과정에서는 이 접근 방식이 생각보다 간단하지 않다는 것을 알게 되었다. 브라우저 환경에서의 음성 입력은 하나의 마이크 자원을 공유하기 때문에, 상담사 측에서 송수신 음성을 명확하게 분리해 다루는 데에는 구조적인 한계가 있었다. 그래서 나는 고객측의 STT를 고객측에서 생성하여 WebRTC 스트림을 통해 상담사한테 보내는 방법을 고안했다. 상태 관리를 통해서 들어오는 STT를 관리했고, 데이터가 들어오면 욕설 유무를 판단하기로 했다.

욕설 유무를 판단하는 것 까지는 성공적이었으나, 우리가 간과한 점이 하나 있었다. 세상엔 너무나도 다양한 사람이 있고, 이 사람들이 어떤 방법으로 상담원을 괴롭힐지 예측이 불가능하다는 것이였다. 욕설을 사용하지 않고도 사람을 괴롭히는 방법은 많다. 그래서 특정 단어를 찾아내는 것이 아니라 문맥전체를 판단했어야 했다. LLM을 사용하면 좋지 않을까 생각하여 아는 선배의 추천을 받아 Ollama를 사용해봤다.

Ollama는 LLM을 직접 실행할 수 있도록 만든 런타임-패키징 도구로, 번거로운 과정없이 도커에 띄워서 쓸 수 있었다. Ollama에서 쓸 수 있는 많은 모델들이 있었는데, 한국어에 강한 LG의 EXAONE 모델을 채택했다. EXAONE은 생각보다 성능이 좋았지만, Whisper을 사용할때와 같은 이유로 결국 다른걸 찾아보게 되었다. 응답이 느려 생기는 병목을 견딜 수 없었다. 

LLM은 문맥 이해에는 강했지만, 실시간 시스템에는 예측 가능한 지연과 비용이 더 중요했기 때문에 LLM이 아닌 다른 방법을 찾아보게 되었고, SmileGate의 kor_unsmile 모델을 알게 되었다.  

unsmile은 한국어 문장을 입력받아 toxicity 여부를 분류하는 텍스트 분류 모델로 욕설이나 폭언, 혐오표현과 같은 부적절한 텍스트를 감지하는 사전 학습된 한국어 모델을 욕설, 혐오 데이터로 파인튜닝한 것이다. LLM 대비 매우 가볍고 CPU 환경에서도 거의 실시간으로 처리가 가능했다. 하지만 unsmile은 가벼운 만큼 한계가 있었는데, 애매한 비꼼이나 특정 맥락에서 문제가 되는 표현을 탐지하기 힘들었으며, 상담에 특화된 것이 아니라 완벽하게 문제를 잡아내진 못했다. 그래서 나는 추가로 상담과는 관련이 전혀 없는 특정 단어들을 리스트로 정리하여 toxicity 결과에 보정을 넣었다.

그렇게 폭력성 판별자체는 성공적이였고, 이렇게 끝나는 듯 싶었으나....

---

## 세 번째 시도

unsmile이 아무리 가볍다 해도 실시간으로 들어오는 음성에 대한 필터링 자체는 불가능에 가까웠다. 왜냐하면 프로젝트 구조상 STT가 만들어지면 그걸 판단하기 시작하고, 이 STT는 상대방의 말이 끝나면 다 만들어지기 때문이다. 사실 우리 프로젝트가 아니더라도, 상대방이 무슨말을 할지 알아야 실시간으로 필터링을 할수 있기 때문에 다른 방법을 생각해봤어야 했다. 

WakeWord를 사용하면 어떨까 싶었지만, 이것도 어차피 음성이 들리고 시작되기 때문에 의미는 없었다. 그래서 음성 필터링을 실시간으로 한다는 것은 불가능함을 깨닫고, 음성 필터링에 대한 문제 정의를 다시했다.

음성 필터링은 발화가 완료되기 전에는 의미 판단이 불가능하다는 점에서 본질적으로 후처리 문제라는 것을 배웠다. 그렇게 다시 방법을 생각해보다가 고객측의 음성은 딜레이를 주어 AI와 STT가 모두 작동할 시간을 벌어보기로 했다.

DelayNode를 통해 대략 3초의 지연을 주었고, 음성이 모두 들리기전 STT를 로컬 AI 서버를 통해 전송하여 폭력성 여부를 판단하고, 폭력성이 나타나면 다음 STT가 오기전의 모든 음성을 차단했다.

DelayNode는 AudioNode를 상속한 Web API 객체로, 브라우저 오디오 엔진 내부에 존재하는 인스턴스다.

Web Audio API는 오디오를 그래프로 처리하는데,

```css
[SourceNode] -> [DelayNode] -> [GainNode] -> [Destination]
```

이러한 구조를 갖고 있다.

SourceNode는 오디오 샘플을 생성, 공급한다. Delay노드는 그샘플을 시간 버퍼로 늦추고, GainNode에서 크기 조절, 음소거를 담당하며 Destination 노드에서 스피커로 내보내는 최종출력을 결정한다.

이렇게 딜레이를 주어 구현한 결과, 고객측의 음성 필터링은 성공적으로 구현 할 수 있었다. 하지만 고객한테 말하는 응답이 3초씩 밀리는건 어쩌면 실시간 상담에서 치명적일 수 있기에 시간을 조금씩 조절하여 최대한 불편함을 줄이고자 하였다.

---

## 전체 흐름

전체흐름은 다음과 같다.

고객의 음성을 STT로 변환 -> WebRTC를 통해 상담사 측에 전달 -> 음성은 N초 딜레이 -> 딜레이 되는 동안 STT의 폭력성 여부판단 -> 상담사 측에 STT 내용과 함께 음성 재생 혹은 필터링

---

## 배운점

문제 해결 방법은 아주 다양하다. 어떻게든 기술적인 한계를 극복하는 방법이 있고, 기술적 한계를 인정하고 새로운 길로 돌아가는 방법도 있다.
급할수록 돌아가라는 옛말은 틀리지 않았다. 넓은 시야를 가지게 된다면, 유여한 대처가 가능하다는 것을 알았다.

 이번 프로젝트는 문제 해결방법과 오디오 스트림에 대해 공부할 수 있는 계기가 되었으며, 문제를 해결하며 경험한 실패와 성공을 통해 더욱 나아가야겠다 생각했다.